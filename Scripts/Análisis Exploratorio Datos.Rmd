---
title: "Análisis accidentes de tráfico NY"
author: "Jon Aguas, Fermín Fernandez y Rubén González"
output:
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    number_sections: true
---

## Cargar librerías y lectura de datos:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(viridis)
library(leaflet)
library(ggplot2)
library(ggbiplot)
library(dplyr)
library(GGally)
library(factoextra)
library(cluster)
library(tsibble)
library(psych)
library(caret)
library(MASS)
library(ca)
library(leaflet)
library(leaflet.extras)
library(stringdist)
library(igraph)
library(tidygraph)
library(ggraph)
library(pROC)
library(patchwork)
```

```{r}
#data <- read.csv("Vehicle_Collisions.csv")
```

## Lectura datos (temporal)

Jon:

```{r}
#data <- read.csv("/DatosJon/UPNA/3º Ciencia de Datos/S6/Analisis Multivariante y Visualizacion de Datos/Trabajo/Datos/Vehicle_Collisions.csv")
```

Rubén:

```{r}
#data <- read.csv("C:/Users/ruben/OneDrive/Escritorio/UPNA/CIENCIA DE DATOS/6ºSemestre/AnalisisMultivarianteVisualizacionDatos/Trabajo Final/Motor_Vehicle_Collisions_-_Crashes.csv")
```

Fermin:

```{r}
data <- read.csv("C:/Users/mintx/Mi unidad/CdD UPNA/.3.2/Analisis multivariante y visualizacion de datos/Trabajo/Vehicle_Collisions.csv")
```

# INTRODUCCION

En nuestro conjunto de datos, se recoje información sobre accidentes de tráfico en la ciudad de Nueva York. La información proviene de la página de datos del gobierno de EEUU, específicamente del apartado correspondiente a la ciudad de Nueva York.

Cada individuo corresponde a un accidente de tráfico, que se representa como una fila en el conjunto de datos. Las columnas incluyen información como la fecha y hora del accidente, la ubicación, el tipo de vehículo, y los factores contribuyentes al accidente entre otros muchos. Los accidentes se han almacenado desde el 1/01/2013 hasta la actualidad.

Son accidentes en los que se ha rellenado un informe policial, por lo que podrían no incluirse algunos accidentes menores o accidentes que no han sido reportados a la policía. Ya que este informe es necesario para los accidentes donde hay fallecidos, o daños de al menos 1000\$.

Además, como la forma de recoger los datos ha ido cambiando a lo largo de los años, es posible que hace años no se almacenase información como puede ser las coordenadas, y que en los últimos años con la implementación de dispositivos electrónicos se almacenen las coordenadas exactas de cada accidente. También habrá diferencias en la información dependiendo de la forma en la que el agente de policia correspondiente haya guardado los datos, ya que no son sensores sin fallo los que determinan la mayoría de variables (como pueden ser el tipo de vehículo o factor contribuyentes de la colisión).

Con todo esto, en nuestro trabajo trataremos de analizar las opciones que nos da el conjunto, e ir mostrando los resultados obtenidos visualmente. Y así poder encontrar patrones, agrupar los datos, o ver si podemos encontrar relación entre las variables que nos permitan entender mejor el problema tratado.

# LIMPIEZA INICIAL Y MUESTREO

En primer lugar, observamos que el dataset contiene más de 2 millones de filas, pero que además muchas de las variables están prácticamente vacías o con información redundante. Además, dada la naturaleza de los datos geolocalizados, hay ejemplos que no contienen esa información y no nos son de ayuda.

Por tanto, vamos a realizar una primera limpieza del dataset, eliminando tanto variables redundantes o con poca información, como ejemplos que no nos aporten la información suficiente.

## Eliminar variables:

-   *ZIP.CODE* (ya tenemos la variable *BOROUGH* que contiene el barrio)
-   *ON.STREET.NAME*, *OFF.STREET.NAME*, *CROSS.STREET.NAME* (es información que ya se almacena en las coordenadas)
-   *VEHICLE.TYPE.CODE.3*, *VEHICLE.TYPE.CODE.4*, *VEHICLE.TYPE.CODE.5* (prácticamente todos son valores nulos)
-   *CONTRIBUTING.FACTOR.VEHICLE.2*, *CONTRIBUTING.FACTOR.VEHICLE.3*, *CONTRIBUTING.FACTOR.VEHICLE.4*, *CONTRIBUTING.FACTOR.VEHICLE.5* (prácticamente todos son valores nulos)
-   *COLLISION_ID* (es un identificador que no aporta información)
-   *LOCATION* (es la combinación exacta de las variables *LATITUDE* Y *LONGITUDE*)

```{r}
data_cleaned <- data |> 
  dplyr::select(-ZIP.CODE, -ON.STREET.NAME, -OFF.STREET.NAME, -CROSS.STREET.NAME, -VEHICLE.TYPE.CODE.3, -VEHICLE.TYPE.CODE.4, -VEHICLE.TYPE.CODE.5, -COLLISION_ID, -CONTRIBUTING.FACTOR.VEHICLE.5, -CONTRIBUTING.FACTOR.VEHICLE.4, -CONTRIBUTING.FACTOR.VEHICLE.3, -CONTRIBUTING.FACTOR.VEHICLE.2,-LOCATION)
```

Con el filtrado realizado, obtenemos un dataset mucho más manejable en cuanto a variables se refiere, que nos permitirá trabajar mejor el problema. Haciendo un pequeño resumen de las variables que nos quedan:

-   CRASH.DATE: Fecha en la que se produció el accidente (Date)
-   CRASH.TIME: Hora en la que se produció el accidente (Time)
-   BOROUGH: Barrio de Nueva York donde sucedió el accidente (String)
-   LATITUDE/LONGITUDE: Coordenadas geográficas (Int)
-   NUMBER.OF.(PERSONS/PEDESTRIANS/CYCLIST/MOTORIST).(KILLED/INJURIED): Número y tipo de personas implicadas en el accidente, diferenciando heridos y fallecidos (Int)
-   CONTRIBUTING.FACTOR.VEHICLE: Causa principal del accidente (String)
-   VEHICLE.TYPE.CODE.1: Tipo del primero vehiculo implicado (String)
-   VEHICLE.TYPE.CODE.2: Tipo del segundo vehiculo implicado (String)

## Eliminar individuos:

-   Individuos que no tengan *LATITUDE* y/o *LONGITUDE* (No nos sirven para localizar el accidente)
-   Individuos que no tengan ningún Vehiculo implicado en *VEHICLE.TYPE.CODE*
-   Individuos que no tengan barrio *BOROUGH* (ya que tenemos los suficientes que si tienen esa información)
-   Individuos que no tengan una razon que haya causado el accidente *CONTRIBUTING.FACTOR.VEHICLE*
-   Individuos con valor de localización erronea (Coordenadas 0) *LONGITUDE*

```{r}
data_cleaned <- data_cleaned |> 
  filter(
    !is.na(LATITUDE) & !is.na(LONGITUDE),
    !(is.na(VEHICLE.TYPE.CODE.1) & is.na(VEHICLE.TYPE.CODE.2)),
    !(VEHICLE.TYPE.CODE.1 == "" & VEHICLE.TYPE.CODE.2 == ""),
    !(BOROUGH == ""),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == "Unspecified"),
    !(CONTRIBUTING.FACTOR.VEHICLE.1 == ""),
    !(LATITUDE==0),
    !(LONGITUDE==0))
```

## Renombrar variables:

Antes de empezar a trabajar con los datos, vamos a renombrar ciertas variables para facilitar la programación.

```{r}
data_cleaned <- data_cleaned |> 
  rename(
    DATE = CRASH.DATE,
    TIME = CRASH.TIME,
    NUM_PERSONS_INJURED = NUMBER.OF.PERSONS.INJURED,
    NUM_PERSONS_KILLED = NUMBER.OF.PERSONS.KILLED,
    NUM_PEDESTRIANS_INJURED = NUMBER.OF.PEDESTRIANS.INJURED,
    NUM_PEDESTRIANS_KILLED = NUMBER.OF.PEDESTRIANS.KILLED,
    NUM_CYCLIST_INJURED = NUMBER.OF.CYCLIST.INJURED,
    NUM_CYCLIST_KILLED = NUMBER.OF.CYCLIST.KILLED,
    NUM_MOTORIST_INJURED = NUMBER.OF.MOTORIST.INJURED,
    NUM_MOTORIST_KILLED = NUMBER.OF.MOTORIST.KILLED,
    CAUSE = CONTRIBUTING.FACTOR.VEHICLE.1,
    VEHICLE_1 = VEHICLE.TYPE.CODE.1,
    VEHICLE_2 = VEHICLE.TYPE.CODE.2
  )
  
```

## Eliminación de filas incompletas:

### Personas afectadas:

Vamos a comprobar si el número de personas heridas o fallecidas coincide con el número de heridos o fallecidos que se han dado en la fila. Para ello, vamos a crear una tabla con los diferentes tipos de heridos y fallecidos, y luego vamos a comprobar si el número total coincide con el número de personas heridas o fallecidas que se han dado en la fila.

```{r}
type_injured <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  pivot_longer(cols = everything(), names_to = 'TYPE_OF_INJURED', values_to = 'NUMBER')

# quiero saber el total de de cada tipo

type_injured |> 
  group_by(TYPE_OF_INJURED) |> 
  summarise(NUMBER = sum(NUMBER)) |> 
  arrange(desc(NUMBER))

# ahora mostramos los indices de los que no coinciden
filas_erroneas <- data_cleaned |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED) |> 
  mutate(ROW_ID = row_number(),
         TOTAL_NUMBER_INJURED = rowSums(across(c(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED))),
         TOTAL_NUMBER_KILLED = rowSums(across(c(NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)))) |>
  dplyr::select(ROW_ID, NUM_PERSONS_INJURED, TOTAL_NUMBER_INJURED, NUM_PERSONS_KILLED, TOTAL_NUMBER_KILLED) |> 
  filter(NUM_PERSONS_INJURED != TOTAL_NUMBER_INJURED | NUM_PERSONS_KILLED != TOTAL_NUMBER_KILLED)

head(filas_erroneas)
```

Podemos ver como existen filas que no cumplen, por ello eliminamos las filas erroneas, aquellas filas cuyo numero de personas heridas o fallecidas no coincide con el total de heridos o fallecidos que se han dado en la fila.

```{r}
indices_erroneos <- filas_erroneas |> 
  pull(ROW_ID)

data_cleaned <- data_cleaned[-indices_erroneos, ]

```

## Eliminacion de valores incorrectos de la variable 'CAUSE':

```{r}
data_cleaned <- data_cleaned |> 
  mutate(CAUSE = recode(CAUSE,
    "Cell Phone (hand-Held)" = "Cell Phone (hand-held)",
    "Drugs (Illegal)" = "Drugs (illegal)",
    "Illnes" = "Illness",
    "Other Electronic Device" = "Other Devices",
    "Drugs (illegal)" = "Drugs (illegal)",
    "Unspecified" = "Unspecified",
    "Pedestrian/Bicyclist/Other Pedestrian Error/Confusion" = "Pedestrian Error/Confusion",
    "Aggressive Driving/Road Rage" = "Aggressive Driving"
  )) |> 
  filter(CAUSE != 1, CAUSE != 80) 
```

## Formatear variable 'DATE':

```{r}
data_cleaned <- data_cleaned |> 
  mutate(DATE = as.Date(DATE, format = "%m/%d/%Y"))
```

## Estandarizar strings de las variables:

Para facilitar el trabajo con los datos y evitar valores duplicados, vamos a estandarizar las variables CAUSE, VEHICLE_1 y VEHICLE_2 convirtiéndolas completamente a mayúsculas.

```{r}
data_cleaned$VEHICLE_1 <- toupper(data_cleaned$VEHICLE_1)
data_cleaned$VEHICLE_2 <- toupper(data_cleaned$VEHICLE_2)
data_cleaned$CAUSE <- toupper(data_cleaned$CAUSE)
```

## Muestreo:

Una vez eliminados los individuos que no son de interes, el dataset se reduce a 888068 filas. Seguimos teniendo demasiadas observaciones, por lo que para trabajar con los datos de forma más comoda hemos decidido hacer un muestreo de 20.000 observaciones. Cabe destacar que si bien para tener los mismos resultados fijamos una semilla, durante el trabajo en ciertos analisis hemos tomado muestras diferentes para asegurarnos de que los resultados eran similares y no había demasiada variación debido a la muestra.

```{r}
set.seed(123)
data_sampled <- sample_n(data_cleaned, 20000)
```

Como a partir de ahora trabajaremos con esta muestra en lugar del dataset completo, por comodidad lo almacenaremos en un nuevo archivo para solo tener que leer este.

```{r}
write.csv(data_sampled, "data_sampled.csv", row.names = FALSE)
write.csv(data_sampled, "App/data_sampled.csv", row.names = FALSE)
```

### Leer muestra:

```{r}
data_sampled <- read.csv("data_sampled.csv")
```

# ANALISIS DESCRIPTIVO

Una vez obtenido el dataset con el que trabajaremos, vamos a visualizar las diferentes variables y sus comportamientos para tratar de entender mejor con que datos estamos trabajando antes de aplicar las diferentes técnicas.

Además, también mostraremos la matriz de correlación entre las variables numéricas, para ver si hay alguna relación entre ellas.

Antes de empezar con las visualizaciones, vamos a crear un tema personalizado para los gráficos que vamos a mostrar. De esta manera, todos los gráficos tendrán el mismo estilo y será más fácil de leer.

## Tema personalizado

Tanto en el informe como en la app, hemos definido un tema personalizado con colores característicos de Nueva York y particularmente los vehiculos, en el que destaca la presencia del amarillo color Taxi, y los grises del asfalto para dar contraste.

```{r}
theme_nyc <- function() {
  theme_minimal(base_family = "Arial") +
    theme(
      plot.background = element_rect(fill = "#F4F4F4", color = NA),
      panel.background = element_rect(fill = "#F4F4F4", color = NA),
      panel.grid.major = element_line(color = "#DADADA"),
      panel.grid.minor = element_blank(),
      axis.title = element_text(color = "#2E2E2E", face = "bold"),
      axis.text = element_text(color = "#2E2E2E"),
      plot.title = element_text(color = "#1F3B73", size = 16, face = "bold", hjust = 0.5),
      plot.subtitle = element_text(color = "#2E2E2E", hjust = 0.5),
      legend.background = element_rect(fill = "#F4F4F4"),
      legend.key = element_rect(fill = "#F4F4F4"),
      legend.text = element_text(color = "#2E2E2E"),
      strip.background = element_rect(fill = "#FFD100"),
      strip.text = element_text(color = "#2E2E2E", face = "bold")
    )
}
```


## Variable 'DATE':

Vamos a hacer una representación dual, en la que combinamos el gráfico de barras y el gráfico de líneas, para mostrar la frecuencia de accidentes según el mes o año. Para ello, vamos a crear una nueva variable que contenga la fecha en formato mes y otra en año.

```{r, warning=FALSE}
data_time <- data_sampled |>
  mutate(DATE = as.Date(DATE, format = "%Y-%m-%d")) |>
  mutate(DATE_MONTH = yearmonth(DATE),
         DATE_YEAR = year(DATE)) |>
  add_count(DATE, name = "FREQ_DAY") |>
  add_count(DATE_MONTH, name = "FREQ_MONTH") |>
  add_count(DATE_YEAR, name = "FREQ_YEAR") |>
  dplyr::select(DATE, DATE_MONTH, DATE_YEAR, FREQ_DAY, FREQ_MONTH, FREQ_YEAR, BOROUGH) |>
  arrange(DATE)


# Frecuendia de datos mensuales
data_time |> 
  group_by(DATE_MONTH) |>
  summarise(FREQ = sum(FREQ_MONTH, na.rm = TRUE), .groups = "drop") |>
    ggplot(aes(x = DATE_MONTH, y = FREQ)) +
    geom_bar(stat = "identity", fill = "#1F3B73", alpha = 0.8) +
    geom_line(color = "#1F3B73", size = 1.2) +
    geom_point(color = "#FF4C4C", size = 2) +
    labs(title = "Frecuencia de Accidentes por Mes", x = "Fecha", y = "Frecuencia") +
    theme_nyc()

# Frecuendia de datos mensuales
data_time |> 
  group_by(DATE_YEAR) |>
  summarise(FREQ = sum(FREQ_YEAR, na.rm = TRUE), .groups = "drop") |>
    ggplot(aes(x = DATE_YEAR, y = FREQ)) +
    geom_bar(stat = "identity", fill = "#1F3B73", alpha = 0.8) +
    geom_line(color = "#1F3B73", size = 1.2) +
    geom_point(color = "#FF4C4C", size = 2) +
    labs(title = "Frecuencia de Accidentes por Año", x = "Fecha", y = "Frecuencia") +
    theme_nyc()
  

```

En estos gráficos observamos como ha ido evolucionando la frecuencia de accidentes a lo largo del tiempo, ya sea con datos acumulados mensuales o anuales. Se puede ver claramente como en el año del covid la frecuencia de accidentes disminuye significativamente, debido a que la gente no se movía tanto como en años anteriores. Sin embargo, la frecuencia de accidentes se mantiene casi constante, incluso llegando a disminuir en en 2022. Esto tambien se puede deber a que a raiz del covid la gente ha cambiado su forma de moverse, y ahora se mueven más en bicicleta o andando, lo que puede hacer que haya más accidentes de este tipo. También, hemos comprobado que después del covid, en EEUU, modificaron leyes de tráfico que hizo que la gente sea mas consciente de la seguridad vial, y por lo tanto, haya menos accidentes.


A continuación, vamos a representar la frecuencia total acumulada por cada mes. Es decir, la suma de frecuencia para cada uno de los meses en todos los años pasados:

```{r, warning=FALSE}
data_time |> 
  group_by(DATE_MONTH) |> 
  summarise(FREQ_MONTH = sum(FREQ_MONTH)) |> 
  mutate(MONTH = month(DATE_MONTH, label = TRUE)) |> 
  ggplot(aes(x = MONTH, y = FREQ_MONTH)) + 
    geom_histogram(stat = "identity", fill = "#1F3B73", alpha = 0.8) +
    labs(title = "Frecuencia de accidentes acumulada por mes", x = "Fecha", y = "Frecuencia") +
    theme_nyc()
```
En este gráfico observamos cuales son los meses que máyor frecuencia de accidentes tienen. Observamos como a principios de verano es cuando más alta es la frecuencia (mayo, junio y julio) tal vez porque la gente se mueve más por el buen tiempo, pero aún teniendo que ir a trabajar. En agosto y septiembre se reduce la frecuencia, tal vez por el efecto de las vacaciones. Por otra parte, sin duda los meses de febrero (por tener 29 días) y abril (tal vez por ser el mes en el que más afecto el confinamiento de 2020) son los que menor frecuencia de accidentes tienen.


## Variable 'BOROUGH':

En el siguiente apartado, vamos a tratar de ver la distribución de los accidentes por barrio, y su evolución con el paso del tiempo:

```{r, warning=FALSE}
data_sampled |> 
  ggplot(aes(x = BOROUGH)) +
  geom_bar(fill = "#1F3B73") +
  geom_text(aes(label = after_stat(count)), stat = "count", vjust = -0.5, size = 3) +
  labs(title = "Distribución de Accidentes por Distrito",
       x = "Barrio", y = "Número de Accidentes") +
  theme_nyc()

```
Observamos como Brooklyn Queens y Manhattan acumulan el mayor número de accidentes. Esto concuerda con que son los barrios más grandes y céntricos de Nueva York. Se observa como Bronx y State Island muestran una frecuencia mucho más inferior, ya que son barrios periféricos.


Vamos a ver la evolución de la frecuencia de accidentes por cada barrio:

```{r, warning=FALSE}
data_time |>
  group_by(DATE = floor_date(DATE, "month"), BOROUGH) |>
  summarise(accidents = sum(FREQ_DAY, na.rm = TRUE), .groups = "drop") |> 
  ggplot(aes(x = DATE, y = accidents, color = BOROUGH)) +
    geom_line(size = 1.1) +
    geom_point(size = 1.5) +
    labs(title = "Evolución Temporal de Accidentes por Barrio",
         x = "Fecha", y = "Número de Accidentes", color = "Barrio") +
    theme_nyc() +
    scale_color_brewer(palette = "Set1") +
    theme(legend.position = "bottom")
```
Aqui observamos el mismo patrón de frecuencia que hemos analizado anteriormente, donde baja drásticamente a partir del covid. Además, parece que el comportamiento de todos los barrios es similar, y aumentan o disminuyen de manera correlacionada. También parece que el barrio Bronx no disminuye tanto la frecuencia a partir de 2020, mostrándose por encima de manhattan varias veces.También podemos observar como Manhattan era el barrio con mayor frecuencia hasta 2016, y desde 2017 hasta 2020 cede ese puesto a Brooklyn y Queens.


## Variables 'LATITUDE' y 'LONGITUDE':

En cuanto a las variables de Localización, al ser un dataset con datos geográficos uno de los análisis importantes que debemos hacer es situando los accidentes en su ubicación correspondiente. Esto nos permite ver como están distrubuidos en la ciudad los accidentes.

En primer lugar, representaremos cada accidente mediante un punto rojo:
```{r, results='asis', echo=FALSE}
data_sampled |> 
  leaflet() |> 
  addTiles() |> 
  addCircleMarkers(
    lng = ~LONGITUDE,
    lat = ~LATITUDE,
    radius = 2,
    color = "red",
    stroke = FALSE,
    fillOpacity = 0.6,
    popup = ~paste("Lat:", LATITUDE, "<br>Lon:", LONGITUDE)
  )
```

Observamos como prácticamente la totalidad del mapa queda cubierta de punto rojos, si bien es cierto que en barrios como Staten Island se aprecia menos densidad, tal y como intuíamos de los histogramas realizados previamente.

Para tratar de ver algo más claro, vamos a representar un mapa de calor, en el que las zonas más calientes representan aquellos accidentes en los que ha habido heridos o fallecidos (que ponderan de forma más significativa):

```{r}
leaflet(data_sampled) %>%
  addTiles() %>%
  setView(lng = -73.9, lat = 40.7128, zoom = 10) %>%
  addHeatmap(
    lng = ~LONGITUDE,
    lat = ~LATITUDE,
    intensity = ~ 5*NUM_PERSONS_KILLED + NUM_PERSONS_INJURED,
    radius = 15,
    blur = 20,
    max = 0.05,
    group = "heat"
  )
```

Este mapa ya nos proporciona información más interesante. Por un lado, podemos observar como aunque muchos accidentes se producen en Manhattan, no corresponde con una alta densidad de heridos o fallecidos como podría esperarse. esto se debe tal vez a que los accidentes son en ciudad a una baja velocidad.Sin embargo, en barrios como Staten Island observamos una amyor densidad de accidentes con heridos, pese a que era una de las zonas con menor frecuencia de accidentes.

En la aplicación hemos desarrollado los mapas de forma interactiva, pudiendo aplicar diferentes filtros y intervalos de tiempo variables, para poder ver de una forma más clara y dinámica la evolución de los accidentes a lo largo del


## Variables de Conteo de Personas Afectadas:

Ahora, podemos comprobar el número de peatones, ciclistas y motoristas que han resultado heridos o muertos en los accidentes, junto al total de heridos y muertos:

```{r, fig.width=15, warning=FALSE}
    datos_resumen <- data_sampled |> 
      summarise(
        Total_Heridos = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
        Heridos_Pedestrians = sum(NUM_PEDESTRIANS_INJURED, na.rm = TRUE),
        Heridos_Cyclists = sum(NUM_CYCLIST_INJURED, na.rm = TRUE),
        Heridos_Motorists = sum(NUM_MOTORIST_INJURED, na.rm = TRUE),
        Total_Muertos = sum(NUM_PERSONS_KILLED, na.rm = TRUE),
        Muertos_Pedestrians = sum(NUM_PEDESTRIANS_KILLED, na.rm = TRUE),
        Muertos_Cyclists = sum(NUM_CYCLIST_KILLED, na.rm = TRUE),
        Muertos_Motorists = sum(NUM_MOTORIST_KILLED, na.rm = TRUE)
      )
    
    datos_long <- datos_resumen |> 
      pivot_longer(cols = everything(),
                   names_to = c("Estado", "Tipo"),
                   names_sep = "_",
                   values_to = "Total")
    
    ggplot(datos_long, aes(x = Tipo, y = Total, fill = Tipo)) +
      geom_col(show.legend = TRUE) +
      geom_text(aes(label = Total), vjust = -0.5, size = 4, color = "#2E2E2E") +
      facet_wrap(~Estado, scales = "free_y") +
      labs(
        title = "Total de personas heridas y muertas por tipo",
        x = NULL,
        y = "Total",
        fill = "Tipo de persona"
      ) +
      theme_nyc() +
      theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "bottom"
      )
```

En este gráfico observamos que en general, la frecuencia de heridos es mucho mayor respecto a fallecidos (6472 frente a 23). También observamos que el grupo más vulnerable son los motoristas, tanto en cuanto a heridos como a fallecidos se refiere. Seguidos de los peatones y ciclistas. Si bien, considerando que el número de ciclistas es muy inferior al de peatones, y apenas son la mitad de heridos, tal vez podríamos decir que ser ciclista si que conlleva un mayor riesgo de resultar herido en proporción.


Además, pese a ser variables enteras como son numéricas, puede ser interesante mostrar un histograma de cada una de ellas para ver si hay normalidad en los datos.:
```{r, fig.width=15, message=FALSE, warning=FALSE}
vars <- c(
  "NUM_PERSONS_INJURED", "NUM_PEDESTRIANS_INJURED", "NUM_CYCLIST_INJURED", "NUM_MOTORIST_INJURED",
  "NUM_PERSONS_KILLED", "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_KILLED"
)

hist_list <- lapply(vars, function(var) {
  ggplot(data_sampled, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "#1F77B4", color = "white") +
    labs(title = var, x = NULL, y = NULL) +
    theme_nyc()
})

wrap_plots(hist_list, ncol = 4)
```
Observamos como la distribución en todas las variables llega al máximo en el 0, y va reduciéndose drásticamente. Lo cual es comprensible ya que se trata del número de personas heridas en accidentes, y en los accidentes leves no suele haber ningún herido.

Esto nos indica que no tenemos normalidad, lo cual habrá que tenerlo en cuenta a la hora de aplicar diferentes técnicas.


En el siguiente apartado, veremos la matriz de correlaciones entre todas las variables de conteo de personas afectadas. Como no tenemos normalidad, aplicaremos la matriz de correlaciones de Spearman:
```{r, fig.width=15, message=FALSE, warning=FALSE}
data_short <- data_sampled[, c("NUM_PERSONS_INJURED", "NUM_PEDESTRIANS_INJURED", "NUM_CYCLIST_INJURED", "NUM_MOTORIST_INJURED", "NUM_PERSONS_KILLED",   "NUM_PEDESTRIANS_KILLED", "NUM_CYCLIST_KILLED", "NUM_MOTORIST_KILLED")]


colnames(data_short) <- c("Total Her.", "Peatones Her.", "Ciclistas Her.", "Motoristas Her.", "Total Fall.", "Peatones Fall.", "Ciclistas Fall.", "Motoristas Fall.")

ggpairs(
  data_short,
  title = "Matriz de correlaciones Spearman",
  upper = list(continuous = wrap("cor", size = 4, method = "spearman")),
  lower = list(continuous = wrap("points", alpha = 0.5)),
  diag = list(continuous = wrap("densityDiag"))
) +
  theme_nyc()
```
En conclusión, la matriz de correlaciones solamente muestra relaciones fuertes entre variables agregadas y sus subcomponentes, como entre personas heridas y motoristas heridos (ya que dentro de personas heridas están los motoristas, y además hemos visto que es el tipo que más contribuye). Sin embargo esta correlación es evidente, y no aporta ninguna información de valor.

En cuanto a las variables por separado sin datos repetidos, ninguna muestra correlación significativa con las demás, lo cual sugiere que no se comportan del mismo modo los tipos diferentes de implicados, y que no podemos explicar el número de heridos/fallecidos de ningún tipo basandónos en otro de los tipos.


## Variable 'CAUSE':

Primero, vamos a ver la frecuencia de las distintas causas de los accidentes registrados. Para ello, vamos a crear una tabla con los diferentes factores contribuyentes y su frecuencia:

```{r}
causes_cleaned <- data_sampled |> 
  count(CAUSE, name = "FREQUENCY") |>  
  arrange(desc(FREQUENCY))
```

Nos ayudamos de un gráfico de barras para mostrar las frecuencias.

```{r, fig.width=13}
causes_cleaned |> 
  ggplot(aes(x = reorder(CAUSE, FREQUENCY), y = FREQUENCY)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Frecuencia de Factores Contribuyentes de los Accidentes",
       x = "Causas del Accidente",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```

Se observa como gran porcentaje de accidentes se deben a distracciones. También hay un número considerable de las siguientes causas, pero las causas que tienen una menor frecuencia, apenas tienen accidentes. Esto nos hace pensar en un posible agrupamiento en el futuro usando clusters, para tratar de reducir todas las posibles causas a un número más reducido.


## Variables 'VEHICLE_1' y 'VEHICLE_2':

Por ultimo, analizaremos las variables correspondientes al tipo de vehículo primario y secundario implicados en el accidente. De forma similar a la varaible anterior, es una variable categórica, por lo que primero, vamos a crear una tabla para cada variable con su frecuencia:

```{r}
V1_cleaned <- data_sampled |> 
  count(VEHICLE_1, name = "FREQUENCY_V1") |>  
  arrange(desc(FREQUENCY_V1))

V2_cleaned <- data_sampled |> 
  count(VEHICLE_2, name = "FREQUENCY_V2") |>  
  arrange(desc(FREQUENCY_V2))
```

Ahora vamos a graficar los tipos de vehículos y sus frecuencias para *VEHICLE_1* y *VEHICLE_2*. Para ello, vamos a usar un gráfico de barras. Comenzamos con la primera variable:

```{r, fig.width=13}
V1_cleaned |> 
  filter(FREQUENCY_V1 > 5) |> 
  ggplot(aes(x = reorder(VEHICLE_1, FREQUENCY_V1), y = FREQUENCY_V1)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +
  labs(title = "Frecuencia de Tipos de Vehículos (Primer involucrado)",
       x = "Vehículos",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```
Hemos mostrado solamente los vehiculos que tienen una frecuencia de almenos 5, ya que los inferiores generalmente se tratan de errores de tipografía, y por no extender en exceso el gráfico.


De manera similar, mostramos la segunda variable:

```{r, fig.width=13}
V2_cleaned |> 
  filter(FREQUENCY_V2 > 5, VEHICLE_2 != '') |> 
  ggplot(aes(x = reorder(VEHICLE_2, FREQUENCY_V2), y = FREQUENCY_V2)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  coord_flip() +  # Rota el gráfico para mejor visualización
  labs(title = "Frecuencia de Tipos de Vehículos (Primer involucrado)",
       x = "Vehículos",
       y = "Frecuencia") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=60, hjust=1,size=6),
        axis.title = element_text(size=10),
        title = element_text(size=12),
        axis.title.x=element_blank(),
        legend.title = element_blank()
        )
```
En ambos casos observamos que el vehículo más implicado son los coches Sedan habituales como cabía esperar, y va bajando el número drásticamente al resto de tipos de vehículos.

# METODOLOGÍA MULTIVARIANTE

## PCA

Vamos a realizar un análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos y ver si podemos encontrar patrones en los datos. Sin embargo, lo primero que vamos hacer es comprobar si tiene sentido hacer un pca con los datos numéricos que tenemos, ya que no tenemos demasiadas variables y ya hemos visto que son enteras y sin normalidad. Además en la matriz de correlación ya hemos visto que no hay correlaciones significativas en los datos, lo cual puede complicarnoss realizar las combinaciones para reducir el número de componentes.

Sin embargo, para tomar la decisión con un criterio más preciso, hemos decidido aplicar las metodologías de adecuación de la muestra que vimos para el análisis factorial mediante el test de KMO. Nos puede dar una idea también al trabajar con PCA, ya que también necesitamos correlación entre las variables, y explicar una parte significativa de la varianza en ambos métodos.

Aplicamos el test de KMO sobre las variables numéricas. Un valor menor o igual a 0.5 indica que los datos no son adecuados:
```{r}
datos_numeric <- data_sampled |> 
  dplyr::select(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

KMO(datos_numeric)
```
Observamos el valor MSA general es de 0.47, por lo que no parece interesante realizar un PCA tal y como intuíamos. Sin embargo, vamos a realizar un gráfico de PCA sobre las dos componentes principales, para ver si aun así podemos extraer algún comportamiento interesante.

```{r,message=FALSE, warning=FALSE}
datos_pca <- data_sampled |> 
  select(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

pca_result <- prcomp(datos_pca, center = TRUE, scale. = TRUE)

fviz_pca_var(pca_result, repel = TRUE, labelsize = 3, col.var = "contrib") +
  scale_color_viridis(option = "C", direction = -1) +
  theme_nyc()
```
Observamos que usando dos componentes, el porcentaje de variabilidad explicada apenas supera el 35%, lo cual no es lo ideal. Podemos extraer que las variables de heridos contribuyen mucho más significativamente que las variables sobre fallecidos. También podemos observar que el número de motoristar heridos y fallecidos van en la misma dirección, lo que propone que a mayor numero de heridos mayor número de fallecidos. En cuanto a las otras variables, observamos que toman la dirección más opuesta posible, lo que nos indica que la correlación entre ellas es la mínima posible. Debido a esto también la variabilidad explicada con dos componentes es tan baja.

Vamos a mostrar el gráfico de la varianza explicada por cada componente, para ver como va creciendo hasta el 100% que se dará con las 6 variables:

```{r,message=FALSE, warning=FALSE}
pca_summary <- summary(pca_result)

var_exp_cum <- pca_summary$importance[3, ] * 100

data.frame(Componente = 1:length(var_exp_cum),
           Varianza_Acumulada = var_exp_cum) %>%
  ggplot(aes(x = Componente, y = Varianza_Acumulada)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.6) +
  geom_line(color = "red", size = 1) +
  geom_point(color = "red", size = 3) +
  labs(title = "Varianza acumulada explicada por cada componente",
       x = "Componente Principal",
       y = "Varianza acumulada (%)") +
  theme_nyc()
```
Podemos observar que prácticamente todas las componentes aportan la misma cantidad de variabilidad explicada, debido a que no hay correlacion entre las variables como hemos observado antes. Además, observamos que con 5 componentes apenas llegamos al 85% de variabilidad explicada, por lo que una vez más no parece que podamos reducir la dimensionalidad de forma efectiva mediante pCA, ya que perderíamos demasiada información.

Por último, vamos a probar si es interesante analizar la posibilidad de realizar PCA separando las variables por heridos y muertos en lugar de todos juntos. Realizaremos el test de KMO del mismo modo que ahora, para ver si en estos casos sería interesante con dicha muestra.

### PCA con heridos

```{r}
datos_heridos <- data_sampled |> 
  dplyr::select(NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED)

datos_heridos_scaled <- scale(datos_heridos)

KMO(datos_heridos_scaled)
```

De manera similar al caso general, obtenemos que no es interesante realizar un PCA,ya que el KMO vuelve a rechazar la idea de realizar el PCA, con el mismo valor de 0.47, inferior a 0.5.

#### PCA con muertos

```{r}
datos_heridos <- data_sampled |> 
  dplyr::select(NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

datos_heridos_scaled <- scale(datos_heridos)

KMO(datos_heridos_scaled)
```

Y por último tampoco es interesante con los datos de solamente los fallecidos, obteniendose un KMO de 0.5. Como comentábamos al principio, son resultados esrables al ver la matriz de correlaciones y la forma de ser tan particular de nuestras variables numéricas, por lo que deberemos centrarnos en realizar otro tipo de técnicas más apropiadas a nuestro problema.

Además, teniendo en cuenta estos valores en los test de KMO, no vamos a considerar tampoco la posibilidad de realizar un análisis factorial, ya que KMO esindicador de que no es apropiado.


## Clusterizaciones

### Análisis Cluster Jerárquico en variable 'BOROUGH':

#### 1. Seleccionar variables numéricas relevantes:

```{r}
data_cluster <- data_sampled |> 
  dplyr::select(NUM_PERSONS_INJURED, NUM_PEDESTRIANS_INJURED, NUM_CYCLIST_INJURED, NUM_MOTORIST_INJURED,
         NUM_PERSONS_KILLED, NUM_PEDESTRIANS_KILLED, NUM_CYCLIST_KILLED, NUM_MOTORIST_KILLED)

# Escalar (normalizar) los datos
data_scaled <- scale(data_cluster)
```

#### 2. Determinar el número óptimo de clusters:

```{r}
wss <- vector()
set.seed(123)
for (k in 1:10) {
  km <- kmeans(data_scaled, centers = k, nstart = 5)  # menos repeticiones
  wss[k] <- km$tot.withinss
}

plot(1:10, wss, type = "b", pch = 19,
     xlab = "Número de clusters",
     ylab = "Suma de cuadrados intra-clúster (WSS)",
     main = "Método del codo (optimizado)")

```

#### 3. Realizar el clustering y visualizar:

```{r}
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers = 6, nstart = 25)

# Visualizar clústeres en el espacio PCA
fviz_cluster(kmeans_result, data = data_scaled,
             ellipse.type = "norm",
             geom = "point",
             palette = "jco",
             ggtheme = theme_minimal())

```

Vemos que no se obtiene nada claro ni interesante para analizar. Vamos a probar con otro tipo de clustering, el clustering jerárquico.

#### Clustering jerarquico

```{r}
# Calcular la matriz de distancias
dist_matrix <- dist(data_scaled, method = "euclidean")

# Realizar el clustering jerárquico
hc <- hclust(dist_matrix, method = "complete") # --> Mirar a ver que método es mejor
```


```{r}
# Visualizar el dendrograma
plot(hc, labels = FALSE, hang = -1, main = "Dendrograma de Clustering Jerárquico")

# Cortamos:
rect.hclust(hc, k = 3, border = "red")
clusters <- cutree(hc, k = 3)
table(clusters)
```

Dado que tal y como hemos calculado el clúster jerárquico no obtenemos un resultado interesante, vamos a probar la alternativa de agrupar los datos por barrios.

#### Agrupar los datos por barrios y clusterizar:

```{r}
# Sólo con el total de muertos y heridos:
data_borough_sum <- data_sampled |> 
  group_by(BOROUGH) |> 
  summarise(
    total_injured = sum(NUM_PERSONS_INJURED, na.rm = TRUE),
    total_killed = sum(NUM_PERSONS_KILLED, na.rm = TRUE)
  ) |> 
  na.omit()
```


Escalar los datos:

```{r}
data_scaled <- data_borough_sum |> 
  dplyr::select(-BOROUGH) |> 
  scale()
```

Clustering jerárquico:

```{r}
dist_matrix <- dist(data_scaled)
hc <- hclust(dist_matrix, method = "complete")

# Dendrograma
plot(hc, labels = data_borough_sum$BOROUGH, main = "Dendrograma de barrios (suma total)")
rect.hclust(hc, k = 3, border = "blue")  # Prueba con k = 2, 3, 4, etc.
```

Añadir un gráfico de barras comparando las variables entre los clústeres.

El análisis de clúster jerárquico aplicado a los distritos de Nueva York ha dado como resultado una agrupación en tres clústeres bien diferenciados: por un lado, se agrupan el Bronx y Manhattan; por otro, Brooklyn y Queens; y finalmente, Staten Island queda separado en un grupo propio. Esta clasificación cobra sentido cuando se analiza en función de la densidad de población de cada distrito.

Manhattan, con 27.330 habitantes por kilómetro cuadrado, y el Bronx, con 12.831, presentan las mayores densidades de población entre los distritos analizados. Esta alta concentración de personas suele asociarse a una mayor exposición al tráfico, lo que incrementa la probabilidad de accidentes y, por tanto, de víctimas. Esto justificaría que ambos formen parte del mismo grupo, ya que comparten patrones similares tanto en cuanto a intensidad del tráfico como al número total de heridos y fallecidos.

En un segundo grupo aparecen Brooklyn y Queens, con densidades de población intermedias (14.400 y 8.104,7 habitantes por km² respectivamente). Aunque Brooklyn supera al Bronx en densidad, su comportamiento en términos de accidentes se asemeja más al de Queens, probablemente por la estructura urbana de ambos: grandes áreas residenciales, presencia de autopistas y una combinación de zonas densas y otras más dispersas. Esta mezcla da lugar a un número total de víctimas por accidentes más moderado, lo que explica su asociación en un mismo clúster.

Por último, Staten Island se mantiene como un grupo aislado, con una densidad de apenas 1.782,2 habitantes por km². Este distrito, claramente menos poblado y urbanizado, muestra un patrón de accidentes significativamente distinto, con un número mucho menor de víctimas en comparación con el resto. Su baja densidad y características geográficas más suburbanas justifican su separación en un clúster independiente.

Esta clasificación por clústeres, por tanto, se alinea coherentemente con la densidad de población de cada distrito y permite explicar diferencias en el comportamiento de los accidentes de tráfico a lo largo de la ciudad.

Vamos a añadir un gráfico de barras comparando las variables entre los clústeres:

1. Asignamos los clúster al dataframe:

```{r}
# Cortar el dendrograma para obtener 3 clústeres
clusters <- cutree(hc, k = 3)

# Añadir la columna de clúster al dataframe original
data_borough_sum$cluster <- as.factor(clusters)

```

2. Convertimos a formato largo para ggplot:

```{r}
data_borough_long <- data_borough_sum |> 
  pivot_longer(cols = c(total_injured, total_killed), 
               names_to = "Variable", 
               values_to = "Total")
```

3. Creamos el gráfico de barras:

```{r}
ggplot(data_borough_long, aes(x = Variable, y = Total, fill = cluster)) +
  geom_col(position = "dodge") +
  facet_wrap(~BOROUGH) +
  labs(title = "Comparación de heridos y muertos por clúster y barrio",
       x = "Variable", y = "Total", fill = "Clúster") +
  theme_minimal()

```

Este gráfico demuestra que nuestra hipótesis anterior tiene sentido, ya que el clúster es capaz de agrupar los distintos distritos de una manera muy correcta en función del número total de heridos y muertos.

Por último, añadiremos el shilouette para comprobar si nuestra disposición del clúster es buena o no:

```{r}
# Calcular silueta
sil <- silhouette(clusters, dist(as.dist(dist_matrix)))

# Graficar con estilo personalizado:
fviz_silhouette(sil) +
  ggtitle(paste("Índice de Silueta para k = 3")) +
  theme_minimal()
```

El análisis de la silueta respalda en buena medida la clasificación obtenida. Los grupos formados por el Bronx junto a Manhattan, y Brooklyn junto a Queens, muestran una estructura muy sólida: sus valores de silueta son claramente altos, lo que indica que los distritos dentro de cada clúster se parecen mucho entre sí y están bien separados de los demás. Esto refuerza la idea de que ambos grupos tienen sentido, tanto desde un punto de vista numérico como contextual.

Sin embargo, Staten Island, que ha quedado como un clúster independiente, presenta una silueta bastante más baja —incluso negativa—. Esto quiere decir que, aunque por sus características es razonable considerarlo un caso aparte, los datos no lo separan de forma tan clara como ocurre con los otros clústeres. Aun así, teniendo en cuenta su baja densidad de población y su configuración más suburbana, parece justificado mantenerlo como un grupo propio. Este contraste entre lo que dicen los números y lo que observamos en la realidad muestra lo útil que es combinar el análisis estadístico con la interpretación del contexto para entender mejor los resultados.

### Análisis Cluster Jerárquico variable 'CAUSE':

Vamos a realizar una clusterización de los diferentes factores contribuyentes al accidente. Para ello, vamos a usar el algoritmo K-means, que nos permite agrupar los diferentes factores en diferentes grupos.

Establecemos similitudes entre los nombres de las causas, mediante una matriz de distancias que representa la distancia entre las diferentes causas según su similitud.


```{r}
# Crear matriz de distancias entre nombres de causas
dist_matrix <- stringdistmatrix(causes_cleaned$CAUSE, causes_cleaned$CAUSE, method = "jw")  

# método Jaro-Winkler, o "lv" para Levenshtein
rownames(dist_matrix) <- causes_cleaned$CAUSE
colnames(dist_matrix) <- causes_cleaned$CAUSE
```


Clustering jerarquico

```{r, warning=FALSE}
hc <- hclust(as.dist(dist_matrix), method = "ward.D2")

k <- 8

fviz_dend(x = hc,
          k = k,
          k_colors = c("#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b",  "#e377c2", "#7f7f7f"),
          color_labels_by_k = TRUE,
          rect = TRUE,
          rect_fill = TRUE,
          cex = 0.5,
          main = "Dendrograma - ward",
          xlab = "observaciones",
          ylab = "distancia",
          sub = "")

```

Para un valor de k = 8, es decir, agrupar las causas en 8 grupos, se pueden observar cosas interesantes. En primer lugar, hemos conseguido reducir el número de causas de 52 a 8, lo que nos permite simplificar el análisis. Además, hemos agrupado causas similares entre sí, lo que nos permite ver patrones en los datos. Por ejemplo, el grupo 2 (naranja), prácticamente todas son debido a un defecto, ya sea en el pavimento, los frenos... En general, en todos los grupos hay causas relacionadas.

Vamos a graficar las siluetas para ver que tal realiza la clusterización:

```{r, warning=FALSE}
clusters <- cutree(hc, k = k)

# Calcular silueta
sil <- silhouette(clusters, dist(as.dist(dist_matrix)))

# Graficar con estilo personalizado
fviz_silhouette(sil) +
  ggtitle(paste("Índice de Silueta para k =", k)) +
  theme_minimal(base_family = "Roboto Condensed") +
  theme(
    plot.background = element_rect(fill = "#F4F4F4", color = NA),
    panel.background = element_rect(fill = "#F4F4F4", color = NA),
    plot.title = element_text(color = "#1F3B73", size = 16, face = "bold", hjust = 0.5, family = "Bebas Neue"),
    axis.text = element_text(color = "#2E2E2E", size = 10),
    axis.title = element_text(color = "#2E2E2E", size = 11),
    panel.grid.major = element_line(color = "#CCCCCC"),
    panel.grid.minor = element_blank()
  )
```

Seguún este gráfico, es verdad que hay causas que podrían no estar en el cluster correcto. Sin embargo, hay muchas otras que si estarían bien clasificadas, por lo que nos combiene aplicarlo ya que hemos conseguido reducir el número de causas de 52 a 8. Esto nos facilita el trabajo a la hora de querer utilizar dicha variable.


## Análisis discriminante

Vamos a predecir el distrito al que pertenece un accidente. Para ello vamos a aseguir los siguientes pasos:

### 1. Seleccionar las variables predictoras y agrupar categorías poco frecuentes:

```{r}
# Agrupar las 10 categorías más frecuentes de VEHICLE_1
top_veh_types <- data_sampled |> 
  count(VEHICLE_1, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(VEHICLE_1)

data_sampled <- data_sampled |> 
  mutate(VEHICLE_1 = ifelse(VEHICLE_1 %in% top_veh_types, VEHICLE_1, "OTROS"))

# Agrupar las 10 categorías más frecuentes de VEHICLE_2
top_veh_types <- data_sampled |> 
  count(VEHICLE_2, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(VEHICLE_2)

data_sampled <- data_sampled |> 
  mutate(VEHICLE_2 = ifelse(VEHICLE_2 %in% top_veh_types, VEHICLE_2, "OTROS"))

# Agrupar las 10 categorías más frecuentes de CAUSE
top_causes <- data_sampled |> 
  count(CAUSE, sort = TRUE) |> 
  slice_head(n = 10) |> 
  pull(CAUSE)

data_sampled <- data_sampled |> 
  mutate(CAUSE = ifelse(CAUSE %in% top_causes, CAUSE, "OTROS"))
```

Seleccionamos las variables:

```{r}
data_lda <- data_sampled |> 
  dplyr::select(BOROUGH,
         NUM_PERSONS_INJURED, NUM_PERSONS_KILLED, LONGITUDE, LATITUDE,
         NUM_PEDESTRIANS_INJURED, NUM_PEDESTRIANS_KILLED,
         NUM_CYCLIST_INJURED, NUM_CYCLIST_KILLED,
         NUM_MOTORIST_INJURED, NUM_MOTORIST_KILLED,
         VEHICLE_1, VEHICLE_2, CAUSE) |>
  na.omit()
```


### 2. Convertir las variables categóricas en dummies:

```{r}
data_lda_dummy <- dummyVars(BOROUGH ~ ., data = data_lda)
data_lda_transformed <- predict(data_lda_dummy, newdata = data_lda)
data_lda_final <- data.frame(BOROUGH = data_lda$BOROUGH, data_lda_transformed)
```

### 3. Crear un conjunto de entrenamiento y test:

```{r}
set.seed(123)
train_index <- createDataPartition(data_lda_final$BOROUGH, p = 0.7, list = FALSE)
train_data <- data_lda_final[train_index, ]
test_data <- data_lda_final[-train_index, ]
```

### 4. Aplicamos el análisis discriminante:

```{r}
# Aplicar LDA
lda_model <- lda(BOROUGH ~ ., data = train_data)
lda_model

```

### 5. Evaluamos el modelo:

```{r}
predictions <- predict(lda_model, newdata = test_data)

# Matriz de confusión
table(Predicho = predictions$class, Real = test_data$BOROUGH)
```

### 6. Visualizar las dos primeras funciones discriminantes:

```{r}
lda_df <- data.frame(predictions$x, BOROUGH = test_data$BOROUGH)

ggplot(lda_df, aes(x = LD1, y = LD2, color = BOROUGH)) +
  geom_point(alpha = 0.5) +
  labs(title = "Proyección LDA: Accidentes por distrito",
       x = "Función discriminante 1",
       y = "Función discriminante 2") +
  theme_minimal()
```

Se observa que el modelo obtiene buenos resultados únicamente cuando se incluyen las variables LONGITUDE y LATITUDE. Sin embargo, esto introduce una especie de “trampa”, ya que esas variables están muy relacionadas con la ubicación exacta del accidente, lo que facilita en exceso la predicción. Por ello, para evaluar realmente la capacidad del modelo, resulta más apropiado repetir el análisis excluyendo estas variables y observar cómo se comporta con información menos directa.

Vamos a comprobar entonces qué pasaría si realizaramos el mismo análisis sin esas variables:

### 1.2 Selección de variables:

Seleccionamos las variables, excepto LATITUDE y LONGITUDE:

```{r}
data_lda <- data_sampled |> 
  dplyr::select(BOROUGH,
         NUM_PERSONS_INJURED, NUM_PERSONS_KILLED,
         NUM_PEDESTRIANS_INJURED, NUM_PEDESTRIANS_KILLED,
         NUM_CYCLIST_INJURED, NUM_CYCLIST_KILLED,
         NUM_MOTORIST_INJURED, NUM_MOTORIST_KILLED,
         VEHICLE_1, VEHICLE_2, CAUSE) |>
  na.omit()
```


### 2.2 Convertir las variables categóricas en dummies:

```{r}
data_lda_dummy <- dummyVars(BOROUGH ~ ., data = data_lda)
data_lda_transformed <- predict(data_lda_dummy, newdata = data_lda)
data_lda_final <- data.frame(BOROUGH = data_lda$BOROUGH, data_lda_transformed)
```

### 3.2 Crear un conjunto de entrenamiento y test:

```{r}
set.seed(123)
train_index <- createDataPartition(data_lda_final$BOROUGH, p = 0.7, list = FALSE)
train_data <- data_lda_final[train_index, ]
test_data <- data_lda_final[-train_index, ]
```

### 4.2 Aplicamos el análisis discriminante:

```{r}
# Aplicar LDA
lda_model <- lda(BOROUGH ~ ., data = train_data)
lda_model

```

### 5.2 Evaluamos el modelo:

```{r}
predictions <- predict(lda_model, newdata = test_data)

# Matriz de confusión
table(Predicho = predictions$class, Real = test_data$BOROUGH)
```

### 6.2 Visualizar las dos primeras funciones discriminantes:

```{r}
lda_df <- data.frame(predictions$x, BOROUGH = test_data$BOROUGH)

ggplot(lda_df, aes(x = LD1, y = LD2, color = BOROUGH)) +
  geom_point(alpha = 0.5) +
  labs(title = "Proyección LDA: Accidentes por distrito sin LATITUDE y LONGITUDE",
       x = "Función discriminante 1",
       y = "Función discriminante 2") +
  theme_minimal()
```

Una vez eliminadas las variables LONGITUDE y LATITUDE del análisis, se aprecia una pérdida notable en la calidad de la agrupación, los clústeres resultantes no muestran una estructura tan clara como antes. Esto indica que los grupos formados tienen menos coherencia interna y mayor solapamiento entre ellos. En otras palabras, sin la información geográfica precisa que aportaban esas variables, el modelo tiene más dificultades para diferenciar patrones consistentes en los datos. Este resultado confirma que LONGITUDE y LATITUDE eran variables muy influyentes —quizá demasiado— y que, al eliminarlas, se muestra la verdadera dificultad del problema de segmentación.
 
## Análisis de correspondencia

Para llevar a cabo el análisis de correspondencias, consideramos interesante estudiar si existía alguna relación entre la hora y el día en que ocurren los accidentes y el resto de variables. Para ello, fue necesario calcular e incorporar al conjunto de datos las variables *HOUR* y *DAY_OF_WEEK*:

```{r}
data_sampled <- data_sampled |> 
  mutate(HOUR = as.numeric(format(strptime(TIME, format = "%H:%M"), "%H")),
         DAY_OF_WEEK = weekdays(as.Date(DATE, format = "%Y-%m-%d")))
data_sampled$HOUR <- as.factor(data_sampled$HOUR)
data_sampled$DAY_OF_WEEK <- toupper(data_sampled$DAY_OF_WEEK)
data_sampled$DAY_OF_WEEK <- factor(trimws(data_sampled$DAY_OF_WEEK))  # Eliminar los posibles espacios
```

### 1. Tipo de vehiculo y causa:

```{r, fig.width=10}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$VEHICLE_1, data_sampled$CAUSE)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```

### 2. Tipo de vehiculo y hora:

```{r, fig.width=10}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$VEHICLE_1, data_sampled$HOUR)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```


### 3. Causa del accidente y hora:

```{r, fig.width=10}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$CAUSE, data_sampled$HOUR)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```

### 4. Causa del accidente y día de la semana: 

```{r, fig.width=10}
# Crear una tabla de contingencia
contingency_table <- table(data_sampled$CAUSE, data_sampled$DAY_OF_WEEK)

# Realizar el análisis de correspondencia
ca_result <- ca(contingency_table)

# Resumen del análisis
summary(ca_result)

# Visualizar el gráfico de correspondencia
plot(ca_result)
```

# CONCLUSIONES
```{r}

```

